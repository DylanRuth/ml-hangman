{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "import random\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer, tokenizer_from_json\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, LSTM, Dense, Masking, Dropout, Bidirectional, BatchNormalization\n",
    "from tensorflow.keras.models import load_model\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load training words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "with open('words_250000_train.txt', 'r') as f:\n",
    "    words = f.read().splitlines()\n",
    "\n",
    "train_words, val_words = train_test_split(words, test_size=0.1, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocess Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('_ a _ _ a _ _', 'n'), ('t e t r _ x i a l', 'a'), ('n o _ _ _ _ d', 'e'), ('s c a r l e t l i _ e d', 'n'), ('_ e _ _ _ _ t o', 'i'), ('u p t h r u s _ s', 't'), ('p t _ _ _ _ o _ _ g _ _ a l', 'e'), ('_ _ _ _ _ _ _ _ _', 'a'), ('_ _ _ _ _ _ _ _', 'e'), ('_ o _ _ _ _ _ _ _ _ _ _ _ _', 'e')]\n"
     ]
    }
   ],
   "source": [
    "def character_frequency(word_list):\n",
    "    return Counter(''.join(word_list))\n",
    "\n",
    "def generate_training_examples(word_list):\n",
    "    training_examples = []\n",
    "    for i in range(5):  # Generate 5 training examples for each word\n",
    "        for word in word_list:\n",
    "            chars = list(word)\n",
    "            # Randomly select characters to remove\n",
    "            indices_to_remove = random.sample(range(len(chars)), random.randint(1, len(chars)))\n",
    "            # Collect the removed characters\n",
    "            removed_chars = [chars[i] for i in indices_to_remove]\n",
    "            for i in indices_to_remove:\n",
    "                chars[i] = '_'\n",
    "            highest_freq_char = max(removed_chars, key=lambda char: freq_counter[char])\n",
    "            # Add the modified word and highest frequency character to the training examples\n",
    "            training_examples.append((' '.join(chars), highest_freq_char))\n",
    "    return training_examples\n",
    "\n",
    "# Calculate character frequency from training words\n",
    "freq_counter = character_frequency(train_words)\n",
    "\n",
    "# Generate training examples for training and validation sets\n",
    "training_examples = generate_training_examples(train_words)\n",
    "val_examples = generate_training_examples(val_words)\n",
    "\n",
    "print(training_examples[0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a tokenizer for characters\n",
    "tokenizer = Tokenizer(char_level=True)\n",
    "tokenizer.fit_on_texts(train_words + ['_'])  # Include underscore in tokenizer fitting\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "\n",
    "# Encode current state and next letter\n",
    "def encode_state(state, tokenizer):\n",
    "    state = state.replace(' ', '')\n",
    "    return tokenizer.texts_to_sequences([list(state)])[0]\n",
    "\n",
    "def encode_letter(letter, tokenizer):\n",
    "    return tokenizer.texts_to_sequences([[letter]])[0][0]\n",
    "\n",
    "# Encode and pad training data\n",
    "X_train = [encode_state(state, tokenizer) for state, _ in training_examples]\n",
    "y_train = [encode_letter(next_letter, tokenizer) for _, next_letter in training_examples]\n",
    "X_train = pad_sequences(X_train, maxlen=32, padding='post')\n",
    "X_train = np.array(X_train)\n",
    "y_train = np.array(y_train)\n",
    "\n",
    "# Encode and pad validation data\n",
    "X_val = [encode_state(state, tokenizer) for state, _ in val_examples]\n",
    "y_val = [encode_letter(next_letter, tokenizer) for _, next_letter in val_examples]\n",
    "X_val = pad_sequences(X_val, maxlen=32, padding='post')\n",
    "X_val = np.array(X_val)\n",
    "y_val = np.array(y_val)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, None, 50)          1400      \n",
      "                                                                 \n",
      " masking (Masking)           (None, None, 50)          0         \n",
      "                                                                 \n",
      " bidirectional (Bidirectiona  (None, 64)               21248     \n",
      " l)                                                              \n",
      "                                                                 \n",
      " dense (Dense)               (None, 28)                1820      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 24,468\n",
      "Trainable params: 24,468\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Define the model\n",
    "model = Sequential([\n",
    "    Embedding(input_dim=vocab_size, output_dim=50, input_length=None),\n",
    "    Masking(mask_value=0.0),\n",
    "    Bidirectional(LSTM(32, return_sequences=False)),\n",
    "    # Dropout(0.2),\n",
    "    Dense(vocab_size, activation='softmax')\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "31965/31965 [==============================] - 255s 8ms/step - loss: 1.4574 - accuracy: 0.4816 - val_loss: 1.4397 - val_accuracy: 0.4868\n",
      "Epoch 2/10\n",
      "31965/31965 [==============================] - 251s 8ms/step - loss: 1.4264 - accuracy: 0.4910 - val_loss: 1.4230 - val_accuracy: 0.4916\n",
      "Epoch 3/10\n",
      "31965/31965 [==============================] - 252s 8ms/step - loss: 1.4132 - accuracy: 0.4955 - val_loss: 1.4171 - val_accuracy: 0.4947\n",
      "Epoch 4/10\n",
      "31965/31965 [==============================] - 252s 8ms/step - loss: 1.4039 - accuracy: 0.4983 - val_loss: 1.4083 - val_accuracy: 0.4961\n",
      "Epoch 5/10\n",
      "31965/31965 [==============================] - 252s 8ms/step - loss: 1.3969 - accuracy: 0.5006 - val_loss: 1.4019 - val_accuracy: 0.4976\n",
      "Epoch 6/10\n",
      "31965/31965 [==============================] - 252s 8ms/step - loss: 1.3913 - accuracy: 0.5025 - val_loss: 1.3997 - val_accuracy: 0.4977\n",
      "Epoch 7/10\n",
      "31965/31965 [==============================] - 251s 8ms/step - loss: 1.3874 - accuracy: 0.5038 - val_loss: 1.3956 - val_accuracy: 0.4997\n",
      "Epoch 8/10\n",
      "31965/31965 [==============================] - 252s 8ms/step - loss: 1.3839 - accuracy: 0.5046 - val_loss: 1.3962 - val_accuracy: 0.5015\n",
      "Epoch 9/10\n",
      "31965/31965 [==============================] - 252s 8ms/step - loss: 1.3813 - accuracy: 0.5061 - val_loss: 1.3972 - val_accuracy: 0.5013\n",
      "Epoch 10/10\n",
      "31965/31965 [==============================] - 253s 8ms/step - loss: 1.3792 - accuracy: 0.5063 - val_loss: 1.3937 - val_accuracy: 0.5004\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=10, batch_size=32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('rnn.keras')\n",
    "\n",
    "tokenizer_json = tokenizer.to_json()\n",
    "with open('tokenizer-rnn.json', 'w') as file:\n",
    "    file.write(tokenizer_json)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load and Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_model('rnn.keras')\n",
    "with open('tokenizer-rnn.json', 'r') as file:\n",
    "    tokenizer_json = file.read()\n",
    "tokenizer = tokenizer_from_json(tokenizer_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 19ms/step\n",
      "Next guess: e\n"
     ]
    }
   ],
   "source": [
    "def guess(current_word, guessed_letters, model, tokenizer):\n",
    "    features = encode_state(' '.join(current_word), tokenizer)\n",
    "    features = pad_sequences([features], padding='post')\n",
    "    probabilities = model.predict(features)[0]\n",
    "    \n",
    "    # Sort letters by probability\n",
    "    sorted_indices = np.argsort(probabilities)[::-1]\n",
    "    \n",
    "    for index in sorted_indices:\n",
    "        letter = tokenizer.index_word[index]\n",
    "        if letter not in guessed_letters:\n",
    "            return letter\n",
    "\n",
    "# Example usage\n",
    "current_word = '_ p p l _'\n",
    "guessed_letters = {'r', 'b', 'n', 'u', 'i'}\n",
    "next_guess = guess(current_word, guessed_letters, model, tokenizer)\n",
    "print(f'Next guess: {next_guess}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hangman",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
